{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1437a3b",
   "metadata": {},
   "source": [
    "# Workforce Wellbeing Analysis - Model Training\n",
    "\n",
    "Training three separate models to predict:\n",
    "1. **Burnout Risk Score** (0-1)\n",
    "2. **Wellbeing Score** (0-100)\n",
    "3. **Efficiency Score** (0-100)\n",
    "\n",
    "Using realistic dataset with 300 samples and 110 features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e741e2",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7dc886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b68b50a",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb3d5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the realistic dataset\n",
    "df = pd.read_csv('dataset/realistic_emp_data.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Total features: {len(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981c12e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"âœ“ No missing values found\")\n",
    "else:\n",
    "    print(\"Missing values:\")\n",
    "    print(missing_values[missing_values > 0])\n",
    "\n",
    "# Display data types\n",
    "print(f\"\\nData types:\\n{df.dtypes.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1366cb5d",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8c89c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable statistics\n",
    "target_stats = df[['burnout_risk_score', 'wellbeing_score', 'efficiency_score']].describe()\n",
    "print(\"Target Variable Statistics:\")\n",
    "target_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79438afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target variable distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Burnout Risk Score\n",
    "axes[0].hist(df['burnout_risk_score'], bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Burnout Risk Score Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Burnout Risk Score (0-1)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].axvline(df['burnout_risk_score'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"burnout_risk_score\"].mean():.3f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Wellbeing Score\n",
    "axes[1].hist(df['wellbeing_score'], bins=30, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[1].set_title('Wellbeing Score Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Wellbeing Score (0-100)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].axvline(df['wellbeing_score'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"wellbeing_score\"].mean():.2f}')\n",
    "axes[1].legend()\n",
    "\n",
    "# Efficiency Score\n",
    "axes[2].hist(df['efficiency_score'], bins=30, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[2].set_title('Efficiency Score Distribution', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Efficiency Score (0-100)')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].axvline(df['efficiency_score'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"efficiency_score\"].mean():.2f}')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Target variable distributions plotted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23d4692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Role distribution\n",
    "role_counts = df['role'].value_counts()\n",
    "print(\"Role Distribution:\")\n",
    "print(role_counts)\n",
    "print(f\"\\nRole Percentages:\")\n",
    "print(role_counts / len(df) * 100)\n",
    "\n",
    "# Visualize role distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "role_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title('Employee Role Distribution', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Role')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4a5d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between targets\n",
    "target_corr = df[['burnout_risk_score', 'wellbeing_score', 'efficiency_score']].corr()\n",
    "print(\"Target Variable Correlations:\")\n",
    "print(target_corr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(target_corr, annot=True, fmt='.3f', cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Target Variable Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f489a1c",
   "metadata": {},
   "source": [
    "## 4. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498fa96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and targets\n",
    "X = df.drop(['employee_id', 'burnout_risk_score', 'wellbeing_score', 'efficiency_score'], axis=1)\n",
    "y_burnout = df['burnout_risk_score']\n",
    "y_wellbeing = df['wellbeing_score']\n",
    "y_efficiency = df['efficiency_score']\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"\\nFeature columns:\")\n",
    "print(list(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d772cb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical variable (role)\n",
    "X_encoded = pd.get_dummies(X, columns=['role'], prefix='role', drop_first=False)\n",
    "\n",
    "print(f\"Encoded feature matrix shape: {X_encoded.shape}\")\n",
    "print(f\"Number of features after encoding: {X_encoded.shape[1]}\")\n",
    "print(f\"\\nNew role columns: {[col for col in X_encoded.columns if col.startswith('role_')]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29f8bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature columns for later use\n",
    "feature_columns = list(X_encoded.columns)\n",
    "print(f\"Total features for training: {len(feature_columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cddafc",
   "metadata": {},
   "source": [
    "## 5. Model Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6824ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(X_train, y_train, X_test, y_test, target_name):\n",
    "    \"\"\"\n",
    "    Train multiple models and select the best one using cross-validation.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training target\n",
    "        X_test: Test features\n",
    "        y_test: Test target\n",
    "        target_name: Name of the target variable\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (best_model, model_name, cv_scores, metrics)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training models for: {target_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Define models to test\n",
    "    models = {\n",
    "        'XGBoost': XGBRegressor(random_state=42, n_estimators=100, learning_rate=0.1),\n",
    "        'GradientBoosting': GradientBoostingRegressor(random_state=42, n_estimators=100, learning_rate=0.1),\n",
    "        'RandomForest': RandomForestRegressor(random_state=42, n_estimators=100, max_depth=10)\n",
    "    }\n",
    "    \n",
    "    best_score = -float('inf')\n",
    "    best_model = None\n",
    "    best_model_name = None\n",
    "    all_cv_scores = {}\n",
    "    \n",
    "    # Train and evaluate each model\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        \n",
    "        # Perform 5-fold cross-validation\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
    "        mean_cv_score = cv_scores.mean()\n",
    "        all_cv_scores[name] = cv_scores\n",
    "        \n",
    "        print(f\"  CV RÂ² scores: {cv_scores}\")\n",
    "        print(f\"  Mean CV RÂ²: {mean_cv_score:.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "        \n",
    "        # Track best model\n",
    "        if mean_cv_score > best_score:\n",
    "            best_score = mean_cv_score\n",
    "            best_model = model\n",
    "            best_model_name = name\n",
    "    \n",
    "    # Train best model on full training set\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Best model: {best_model_name} (CV RÂ²: {best_score:.4f})\")\n",
    "    print(f\"Training {best_model_name} on full training set...\")\n",
    "    best_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on train and test sets\n",
    "    train_pred = best_model.predict(X_train)\n",
    "    test_pred = best_model.predict(X_test)\n",
    "    \n",
    "    metrics = {\n",
    "        'model_name': best_model_name,\n",
    "        'cv_mean_r2': float(best_score),\n",
    "        'cv_std_r2': float(all_cv_scores[best_model_name].std()),\n",
    "        'train_r2': float(r2_score(y_train, train_pred)),\n",
    "        'test_r2': float(r2_score(y_test, test_pred)),\n",
    "        'train_rmse': float(np.sqrt(mean_squared_error(y_train, train_pred))),\n",
    "        'test_rmse': float(np.sqrt(mean_squared_error(y_test, test_pred))),\n",
    "        'train_mae': float(mean_absolute_error(y_train, train_pred)),\n",
    "        'test_mae': float(mean_absolute_error(y_test, test_pred))\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nTraining Metrics:\")\n",
    "    print(f\"  RÂ²: {metrics['train_r2']:.4f}\")\n",
    "    print(f\"  RMSE: {metrics['train_rmse']:.4f}\")\n",
    "    print(f\"  MAE: {metrics['train_mae']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nTest Metrics:\")\n",
    "    print(f\"  RÂ²: {metrics['test_r2']:.4f}\")\n",
    "    print(f\"  RMSE: {metrics['test_rmse']:.4f}\")\n",
    "    print(f\"  MAE: {metrics['test_mae']:.4f}\")\n",
    "    \n",
    "    return best_model, best_model_name, all_cv_scores, metrics\n",
    "\n",
    "print(\"âœ“ Model training function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2017467e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(y_true, y_pred, title, target_name):\n",
    "    \"\"\"\n",
    "    Plot actual vs predicted values.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_true, y_pred, alpha=0.5, edgecolor='black')\n",
    "    \n",
    "    # Plot perfect prediction line\n",
    "    min_val = min(y_true.min(), y_pred.min())\n",
    "    max_val = max(y_true.max(), y_pred.max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "    \n",
    "    plt.xlabel(f'Actual {target_name}')\n",
    "    plt.ylabel(f'Predicted {target_name}')\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"âœ“ Prediction plotting function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18802ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance(model, feature_names, top_n=15):\n",
    "    \"\"\"\n",
    "    Extract and plot feature importance.\n",
    "    \"\"\"\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        return importance_df\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def plot_feature_importance(importance_df, title, top_n=15):\n",
    "    \"\"\"\n",
    "    Plot top N most important features.\n",
    "    \"\"\"\n",
    "    if importance_df is not None:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        top_features = importance_df.head(top_n)\n",
    "        plt.barh(range(len(top_features)), top_features['importance'], color='steelblue', edgecolor='black')\n",
    "        plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "        plt.xlabel('Importance')\n",
    "        plt.title(title, fontsize=14, fontweight='bold')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"âœ“ Feature importance functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81b8bbe",
   "metadata": {},
   "source": [
    "## 6. Train Model 1: Burnout Risk Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a822ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for burnout risk model\n",
    "X_train_burnout, X_test_burnout, y_train_burnout, y_test_burnout = train_test_split(\n",
    "    X_encoded, y_burnout, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler_burnout = StandardScaler()\n",
    "X_train_burnout_scaled = scaler_burnout.fit_transform(X_train_burnout)\n",
    "X_test_burnout_scaled = scaler_burnout.transform(X_test_burnout)\n",
    "\n",
    "print(f\"Training set size: {X_train_burnout_scaled.shape[0]}\")\n",
    "print(f\"Test set size: {X_test_burnout_scaled.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87132eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models for burnout risk\n",
    "model_burnout, model_name_burnout, cv_scores_burnout, metrics_burnout = train_models(\n",
    "    X_train_burnout_scaled, y_train_burnout,\n",
    "    X_test_burnout_scaled, y_test_burnout,\n",
    "    'Burnout Risk Score'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a71bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions for burnout risk\n",
    "y_pred_burnout_test = model_burnout.predict(X_test_burnout_scaled)\n",
    "plot_predictions(y_test_burnout, y_pred_burnout_test, \n",
    "                f'Burnout Risk: Actual vs Predicted ({model_name_burnout})',\n",
    "                'Burnout Risk Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e670362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for burnout risk\n",
    "importance_burnout = get_feature_importance(model_burnout, feature_columns)\n",
    "if importance_burnout is not None:\n",
    "    print(\"\\nTop 15 Features for Burnout Risk Prediction:\")\n",
    "    print(importance_burnout.head(15))\n",
    "    plot_feature_importance(importance_burnout, \n",
    "                          f'Top 15 Features for Burnout Risk ({model_name_burnout})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d8bbe2",
   "metadata": {},
   "source": [
    "## 7. Train Model 2: Wellbeing Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bae198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for wellbeing model\n",
    "X_train_wellbeing, X_test_wellbeing, y_train_wellbeing, y_test_wellbeing = train_test_split(\n",
    "    X_encoded, y_wellbeing, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler_wellbeing = StandardScaler()\n",
    "X_train_wellbeing_scaled = scaler_wellbeing.fit_transform(X_train_wellbeing)\n",
    "X_test_wellbeing_scaled = scaler_wellbeing.transform(X_test_wellbeing)\n",
    "\n",
    "print(f\"Training set size: {X_train_wellbeing_scaled.shape[0]}\")\n",
    "print(f\"Test set size: {X_test_wellbeing_scaled.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f40e167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models for wellbeing\n",
    "model_wellbeing, model_name_wellbeing, cv_scores_wellbeing, metrics_wellbeing = train_models(\n",
    "    X_train_wellbeing_scaled, y_train_wellbeing,\n",
    "    X_test_wellbeing_scaled, y_test_wellbeing,\n",
    "    'Wellbeing Score'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6fca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions for wellbeing\n",
    "y_pred_wellbeing_test = model_wellbeing.predict(X_test_wellbeing_scaled)\n",
    "plot_predictions(y_test_wellbeing, y_pred_wellbeing_test,\n",
    "                f'Wellbeing: Actual vs Predicted ({model_name_wellbeing})',\n",
    "                'Wellbeing Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cf3ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for wellbeing\n",
    "importance_wellbeing = get_feature_importance(model_wellbeing, feature_columns)\n",
    "if importance_wellbeing is not None:\n",
    "    print(\"\\nTop 15 Features for Wellbeing Prediction:\")\n",
    "    print(importance_wellbeing.head(15))\n",
    "    plot_feature_importance(importance_wellbeing,\n",
    "                          f'Top 15 Features for Wellbeing ({model_name_wellbeing})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e82ee7",
   "metadata": {},
   "source": [
    "## 8. Train Model 3: Efficiency Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc8c8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for efficiency model\n",
    "X_train_efficiency, X_test_efficiency, y_train_efficiency, y_test_efficiency = train_test_split(\n",
    "    X_encoded, y_efficiency, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler_efficiency = StandardScaler()\n",
    "X_train_efficiency_scaled = scaler_efficiency.fit_transform(X_train_efficiency)\n",
    "X_test_efficiency_scaled = scaler_efficiency.transform(X_test_efficiency)\n",
    "\n",
    "print(f\"Training set size: {X_train_efficiency_scaled.shape[0]}\")\n",
    "print(f\"Test set size: {X_test_efficiency_scaled.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424b7e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models for efficiency\n",
    "model_efficiency, model_name_efficiency, cv_scores_efficiency, metrics_efficiency = train_models(\n",
    "    X_train_efficiency_scaled, y_train_efficiency,\n",
    "    X_test_efficiency_scaled, y_test_efficiency,\n",
    "    'Efficiency Score'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e32cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions for efficiency\n",
    "y_pred_efficiency_test = model_efficiency.predict(X_test_efficiency_scaled)\n",
    "plot_predictions(y_test_efficiency, y_pred_efficiency_test,\n",
    "                f'Efficiency: Actual vs Predicted ({model_name_efficiency})',\n",
    "                'Efficiency Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb49973a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for efficiency\n",
    "importance_efficiency = get_feature_importance(model_efficiency, feature_columns)\n",
    "if importance_efficiency is not None:\n",
    "    print(\"\\nTop 15 Features for Efficiency Prediction:\")\n",
    "    print(importance_efficiency.head(15))\n",
    "    plot_feature_importance(importance_efficiency,\n",
    "                          f'Top 15 Features for Efficiency ({model_name_efficiency})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e485341",
   "metadata": {},
   "source": [
    "## 9. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64b1dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Burnout Risk', 'Wellbeing', 'Efficiency'],\n",
    "    'Algorithm': [model_name_burnout, model_name_wellbeing, model_name_efficiency],\n",
    "    'CV RÂ²': [metrics_burnout['cv_mean_r2'], metrics_wellbeing['cv_mean_r2'], metrics_efficiency['cv_mean_r2']],\n",
    "    'Train RÂ²': [metrics_burnout['train_r2'], metrics_wellbeing['train_r2'], metrics_efficiency['train_r2']],\n",
    "    'Test RÂ²': [metrics_burnout['test_r2'], metrics_wellbeing['test_r2'], metrics_efficiency['test_r2']],\n",
    "    'Test RMSE': [metrics_burnout['test_rmse'], metrics_wellbeing['test_rmse'], metrics_efficiency['test_rmse']],\n",
    "    'Test MAE': [metrics_burnout['test_mae'], metrics_wellbeing['test_mae'], metrics_efficiency['test_mae']]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0301f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# RÂ² comparison\n",
    "x_pos = np.arange(len(comparison_df))\n",
    "width = 0.25\n",
    "\n",
    "axes[0].bar(x_pos - width, comparison_df['CV RÂ²'], width, label='CV RÂ²', alpha=0.8)\n",
    "axes[0].bar(x_pos, comparison_df['Train RÂ²'], width, label='Train RÂ²', alpha=0.8)\n",
    "axes[0].bar(x_pos + width, comparison_df['Test RÂ²'], width, label='Test RÂ²', alpha=0.8)\n",
    "axes[0].set_xlabel('Model')\n",
    "axes[0].set_ylabel('RÂ² Score')\n",
    "axes[0].set_title('Model Performance Comparison (RÂ²)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(comparison_df['Model'], rotation=45, ha='right')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "\n",
    "# Error comparison\n",
    "axes[1].bar(x_pos - width/2, comparison_df['Test RMSE'], width, label='RMSE', alpha=0.8)\n",
    "axes[1].bar(x_pos + width/2, comparison_df['Test MAE'], width, label='MAE', alpha=0.8)\n",
    "axes[1].set_xlabel('Model')\n",
    "axes[1].set_ylabel('Error')\n",
    "axes[1].set_title('Model Error Comparison', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(comparison_df['Model'], rotation=45, ha='right')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8cf1aa",
   "metadata": {},
   "source": [
    "## 10. Save Models and Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632f14dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "import os\n",
    "\n",
    "output_dir = 'model_realistic'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Created output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90691239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "joblib.dump(model_burnout, f'{output_dir}/burnout_risk_model.pkl')\n",
    "joblib.dump(model_wellbeing, f'{output_dir}/wellbeing_model.pkl')\n",
    "joblib.dump(model_efficiency, f'{output_dir}/efficiency_model.pkl')\n",
    "\n",
    "print(\"âœ“ Models saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb6229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save scalers\n",
    "joblib.dump(scaler_burnout, f'{output_dir}/burnout_risk_scaler.pkl')\n",
    "joblib.dump(scaler_wellbeing, f'{output_dir}/wellbeing_scaler.pkl')\n",
    "joblib.dump(scaler_efficiency, f'{output_dir}/efficiency_scaler.pkl')\n",
    "\n",
    "print(\"âœ“ Scalers saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aff319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature columns\n",
    "with open(f'{output_dir}/feature_columns.json', 'w') as f:\n",
    "    json.dump(feature_columns, f, indent=2)\n",
    "\n",
    "print(\"âœ“ Feature columns saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989966b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature importance\n",
    "if importance_burnout is not None:\n",
    "    importance_burnout.to_csv(f'{output_dir}/burnout_risk_feature_importance.csv', index=False)\n",
    "\n",
    "if importance_wellbeing is not None:\n",
    "    importance_wellbeing.to_csv(f'{output_dir}/wellbeing_feature_importance.csv', index=False)\n",
    "\n",
    "if importance_efficiency is not None:\n",
    "    importance_efficiency.to_csv(f'{output_dir}/efficiency_feature_importance.csv', index=False)\n",
    "\n",
    "print(\"âœ“ Feature importance saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72c970c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model metrics\n",
    "all_metrics = {\n",
    "    'burnout_risk': metrics_burnout,\n",
    "    'wellbeing': metrics_wellbeing,\n",
    "    'efficiency': metrics_efficiency,\n",
    "    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'dataset': 'realistic_emp_data.csv',\n",
    "    'total_samples': len(df),\n",
    "    'total_features': len(feature_columns),\n",
    "    'test_size': 0.2\n",
    "}\n",
    "\n",
    "with open(f'{output_dir}/model_metrics.json', 'w') as f:\n",
    "    json.dump(all_metrics, f, indent=2)\n",
    "\n",
    "print(\"âœ“ Model metrics saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150bfb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training summary report\n",
    "summary_report = f\"\"\"\n",
    "{'='*80}\n",
    "WORKFORCE WELLBEING ANALYSIS - MODEL TRAINING SUMMARY\n",
    "{'='*80}\n",
    "\n",
    "Training Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Dataset: realistic_emp_data.csv\n",
    "Total Samples: {len(df)}\n",
    "Training Samples: {len(X_train_burnout)}\n",
    "Test Samples: {len(X_test_burnout)}\n",
    "Total Features: {len(feature_columns)}\n",
    "\n",
    "{'='*80}\n",
    "MODEL 1: BURNOUT RISK SCORE\n",
    "{'='*80}\n",
    "Algorithm: {model_name_burnout}\n",
    "Cross-Validation RÂ²: {metrics_burnout['cv_mean_r2']:.4f} (+/- {metrics_burnout['cv_std_r2']:.4f})\n",
    "Training RÂ²: {metrics_burnout['train_r2']:.4f}\n",
    "Test RÂ²: {metrics_burnout['test_r2']:.4f}\n",
    "Test RMSE: {metrics_burnout['test_rmse']:.4f}\n",
    "Test MAE: {metrics_burnout['test_mae']:.4f}\n",
    "\n",
    "Top 5 Important Features:\n",
    "{importance_burnout.head(5).to_string(index=False) if importance_burnout is not None else 'N/A'}\n",
    "\n",
    "{'='*80}\n",
    "MODEL 2: WELLBEING SCORE\n",
    "{'='*80}\n",
    "Algorithm: {model_name_wellbeing}\n",
    "Cross-Validation RÂ²: {metrics_wellbeing['cv_mean_r2']:.4f} (+/- {metrics_wellbeing['cv_std_r2']:.4f})\n",
    "Training RÂ²: {metrics_wellbeing['train_r2']:.4f}\n",
    "Test RÂ²: {metrics_wellbeing['test_r2']:.4f}\n",
    "Test RMSE: {metrics_wellbeing['test_rmse']:.4f}\n",
    "Test MAE: {metrics_wellbeing['test_mae']:.4f}\n",
    "\n",
    "Top 5 Important Features:\n",
    "{importance_wellbeing.head(5).to_string(index=False) if importance_wellbeing is not None else 'N/A'}\n",
    "\n",
    "{'='*80}\n",
    "MODEL 3: EFFICIENCY SCORE\n",
    "{'='*80}\n",
    "Algorithm: {model_name_efficiency}\n",
    "Cross-Validation RÂ²: {metrics_efficiency['cv_mean_r2']:.4f} (+/- {metrics_efficiency['cv_std_r2']:.4f})\n",
    "Training RÂ²: {metrics_efficiency['train_r2']:.4f}\n",
    "Test RÂ²: {metrics_efficiency['test_r2']:.4f}\n",
    "Test RMSE: {metrics_efficiency['test_rmse']:.4f}\n",
    "Test MAE: {metrics_efficiency['test_mae']:.4f}\n",
    "\n",
    "Top 5 Important Features:\n",
    "{importance_efficiency.head(5).to_string(index=False) if importance_efficiency is not None else 'N/A'}\n",
    "\n",
    "{'='*80}\n",
    "FILES SAVED\n",
    "{'='*80}\n",
    "- burnout_risk_model.pkl\n",
    "- wellbeing_model.pkl\n",
    "- efficiency_model.pkl\n",
    "- burnout_risk_scaler.pkl\n",
    "- wellbeing_scaler.pkl\n",
    "- efficiency_scaler.pkl\n",
    "- burnout_risk_feature_importance.csv\n",
    "- wellbeing_feature_importance.csv\n",
    "- efficiency_feature_importance.csv\n",
    "- feature_columns.json\n",
    "- model_metrics.json\n",
    "- training_summary.txt\n",
    "\n",
    "{'='*80}\n",
    "TRAINING COMPLETE\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "# Save summary report\n",
    "with open(f'{output_dir}/training_summary.txt', 'w') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(summary_report)\n",
    "print(\"\\nâœ“ Training summary saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff24aa8",
   "metadata": {},
   "source": [
    "## 11. Model Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32082c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load models and make predictions\n",
    "def load_models_and_predict(employee_data):\n",
    "    \"\"\"\n",
    "    Load trained models and make predictions for a single employee.\n",
    "    \n",
    "    Args:\n",
    "        employee_data: Dictionary with employee features\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with predictions\n",
    "    \"\"\"\n",
    "    # Load models and scalers\n",
    "    model_burnout_loaded = joblib.load(f'{output_dir}/burnout_risk_model.pkl')\n",
    "    model_wellbeing_loaded = joblib.load(f'{output_dir}/wellbeing_model.pkl')\n",
    "    model_efficiency_loaded = joblib.load(f'{output_dir}/efficiency_model.pkl')\n",
    "    \n",
    "    scaler_burnout_loaded = joblib.load(f'{output_dir}/burnout_risk_scaler.pkl')\n",
    "    scaler_wellbeing_loaded = joblib.load(f'{output_dir}/wellbeing_scaler.pkl')\n",
    "    scaler_efficiency_loaded = joblib.load(f'{output_dir}/efficiency_scaler.pkl')\n",
    "    \n",
    "    # Load feature columns\n",
    "    with open(f'{output_dir}/feature_columns.json', 'r') as f:\n",
    "        feature_cols = json.load(f)\n",
    "    \n",
    "    # Prepare features\n",
    "    employee_df = pd.DataFrame([employee_data])\n",
    "    employee_encoded = pd.get_dummies(employee_df, columns=['role'], prefix='role', drop_first=False)\n",
    "    \n",
    "    # Ensure all features are present\n",
    "    for col in feature_cols:\n",
    "        if col not in employee_encoded.columns:\n",
    "            employee_encoded[col] = 0\n",
    "    \n",
    "    employee_encoded = employee_encoded[feature_cols]\n",
    "    \n",
    "    # Scale features\n",
    "    employee_scaled_burnout = scaler_burnout_loaded.transform(employee_encoded)\n",
    "    employee_scaled_wellbeing = scaler_wellbeing_loaded.transform(employee_encoded)\n",
    "    employee_scaled_efficiency = scaler_efficiency_loaded.transform(employee_encoded)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = {\n",
    "        'burnout_risk_score': float(model_burnout_loaded.predict(employee_scaled_burnout)[0]),\n",
    "        'wellbeing_score': float(model_wellbeing_loaded.predict(employee_scaled_wellbeing)[0]),\n",
    "        'efficiency_score': float(model_efficiency_loaded.predict(employee_scaled_efficiency)[0])\n",
    "    }\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "print(\"âœ“ Prediction function defined\")\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"predictions = load_models_and_predict(employee_data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafd1585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample employee from test set\n",
    "sample_idx = 0\n",
    "sample_employee = X_test_burnout.iloc[sample_idx].to_dict()\n",
    "\n",
    "print(\"Sample Employee Features (first 10):\")\n",
    "for i, (key, value) in enumerate(list(sample_employee.items())[:10]):\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"  ...\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = load_models_and_predict(sample_employee)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREDICTIONS FOR SAMPLE EMPLOYEE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Burnout Risk Score: {predictions['burnout_risk_score']:.4f}\")\n",
    "print(f\"Wellbeing Score: {predictions['wellbeing_score']:.2f}\")\n",
    "print(f\"Efficiency Score: {predictions['efficiency_score']:.2f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compare with actual values\n",
    "print(\"\\nActual Values:\")\n",
    "print(f\"Burnout Risk Score: {y_test_burnout.iloc[sample_idx]:.4f}\")\n",
    "print(f\"Wellbeing Score: {y_test_wellbeing.iloc[sample_idx]:.2f}\")\n",
    "print(f\"Efficiency Score: {y_test_efficiency.iloc[sample_idx]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3d2389",
   "metadata": {},
   "source": [
    "## Training Complete! ðŸŽ‰\n",
    "\n",
    "All three models have been successfully trained and saved to the `model_realistic/` directory.\n",
    "\n",
    "### Next Steps:\n",
    "1. Review model performance metrics above\n",
    "2. Integrate models into your API for real-time predictions\n",
    "3. Collect real data from APIs to further improve models\n",
    "4. Deploy models to production environment"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
